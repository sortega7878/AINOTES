{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Further Pre-Processing\n",
    "This notebook does further pre-processing of data to be ready for input into machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "[2.1. Setup](#1.)<br>\n",
    "[2.1.1 Loading libraries](#1.1)<br>\n",
    "[2.1.2 Setting data directories](#1.2)<br>\n",
    "[2.1.3 Defining functions](#1.3)<br>\n",
    "\n",
    "[2.2. Further Pre-processing](#2.)<br>\n",
    "[2.2.1 Reading in train, validation, and test data sets](#2.1)<br>\n",
    "[2.2.2 Scaling the spectrograms for min max](#2.2)<br>\n",
    "[2.2.3 Setting genre classes](#2.3)<br>\n",
    "\n",
    "[2.3. Saving Pre-Processed Data](#3.)<br>\n",
    "[2.3.1 Shuffling the data and saving as .npy files](#3.1)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Setup <a class=\"anchor\" id=\"1.\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Loading libraries <a class=\"anchor\" id=\"1.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-1.19.2-cp38-cp38-win_amd64.whl (13.0 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.19.2\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.1.2-cp38-cp38-win_amd64.whl (9.6 MB)\n",
      "Collecting pytz>=2017.2\n",
      "  Downloading pytz-2020.1-py2.py3-none-any.whl (510 kB)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\miniconda\\lib\\site-packages (from pandas) (1.19.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\miniconda\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\miniconda\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.1.2 pytz-2020.1\n",
      "Collecting librosa\n",
      "  Downloading librosa-0.8.0.tar.gz (183 kB)\n",
      "Collecting audioread>=2.0.0\n",
      "  Downloading audioread-2.1.8.tar.gz (21 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\miniconda\\lib\\site-packages (from librosa) (1.19.2)\n",
      "Collecting scipy>=1.0.0\n",
      "  Downloading scipy-1.5.2-cp38-cp38-win_amd64.whl (31.4 MB)\n",
      "Collecting scikit-learn!=0.19.0,>=0.14.0\n",
      "  Downloading scikit_learn-0.23.2-cp38-cp38-win_amd64.whl (6.8 MB)\n",
      "Collecting joblib>=0.14\n",
      "  Downloading joblib-0.16.0-py3-none-any.whl (300 kB)\n",
      "Requirement already satisfied: decorator>=3.0.0 in c:\\miniconda\\lib\\site-packages (from librosa) (4.4.2)\n",
      "Collecting resampy>=0.2.2\n",
      "  Downloading resampy-0.2.2.tar.gz (323 kB)\n",
      "Collecting numba>=0.43.0\n",
      "  Downloading numba-0.51.2-cp38-cp38-win_amd64.whl (2.2 MB)\n",
      "Collecting soundfile>=0.9.0\n",
      "  Downloading SoundFile-0.10.3.post1-py2.py3.cp26.cp27.cp32.cp33.cp34.cp35.cp36.pp27.pp32.pp33-none-win_amd64.whl (689 kB)\n",
      "Collecting pooch>=1.0\n",
      "  Downloading pooch-1.2.0-py3-none-any.whl (47 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: six>=1.3 in c:\\miniconda\\lib\\site-packages (from resampy>=0.2.2->librosa) (1.15.0)\n",
      "Requirement already satisfied: setuptools in c:\\miniconda\\lib\\site-packages (from numba>=0.43.0->librosa) (49.2.0.post20200714)\n",
      "Collecting llvmlite<0.35,>=0.34.0.dev0\n",
      "  Downloading llvmlite-0.34.0-cp38-cp38-win_amd64.whl (15.9 MB)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\miniconda\\lib\\site-packages (from soundfile>=0.9.0->librosa) (1.14.0)\n",
      "Requirement already satisfied: requests in c:\\miniconda\\lib\\site-packages (from pooch>=1.0->librosa) (2.24.0)\n",
      "Requirement already satisfied: packaging in c:\\miniconda\\lib\\site-packages (from pooch>=1.0->librosa) (20.4)\n",
      "Collecting appdirs\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: pycparser in c:\\miniconda\\lib\\site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa) (2.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\miniconda\\lib\\site-packages (from requests->pooch>=1.0->librosa) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\miniconda\\lib\\site-packages (from requests->pooch>=1.0->librosa) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\miniconda\\lib\\site-packages (from requests->pooch>=1.0->librosa) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\miniconda\\lib\\site-packages (from requests->pooch>=1.0->librosa) (1.25.10)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\miniconda\\lib\\site-packages (from packaging->pooch>=1.0->librosa) (2.4.7)\n",
      "Building wheels for collected packages: librosa, audioread, resampy\n",
      "  Building wheel for librosa (setup.py): started\n",
      "  Building wheel for librosa (setup.py): finished with status 'done'\n",
      "  Created wheel for librosa: filename=librosa-0.8.0-py3-none-any.whl size=201381 sha256=3d6b202c1c1bc96cfab801a51c2c02d23d55595f1a9e6d37749bdf420841add6\n",
      "  Stored in directory: c:\\users\\sortega78\\appdata\\local\\pip\\cache\\wheels\\aa\\5a\\92\\d52f6f8560ff05a2525e6030a1903412df876714241fb76802\n",
      "  Building wheel for audioread (setup.py): started\n",
      "  Building wheel for audioread (setup.py): finished with status 'done'\n",
      "  Created wheel for audioread: filename=audioread-2.1.8-py3-none-any.whl size=23095 sha256=c03af77a43c0837ea055058e8a78c88ca88650cb4ab928e724fd157b957a6efb\n",
      "  Stored in directory: c:\\users\\sortega78\\appdata\\local\\pip\\cache\\wheels\\9b\\94\\80\\3673f65684ab97e08999d1460fc1b238df7701805c739791cd\n",
      "  Building wheel for resampy (setup.py): started\n",
      "  Building wheel for resampy (setup.py): finished with status 'done'\n",
      "  Created wheel for resampy: filename=resampy-0.2.2-py3-none-any.whl size=320723 sha256=f5b36a29f4a9bf24cd210bdc9fce1f195731545a9f4d7e0ec16a7e871f9414cb\n",
      "  Stored in directory: c:\\users\\sortega78\\appdata\\local\\pip\\cache\\wheels\\6f\\d1\\5d\\f13da53b1dcbc2624ff548456c9ffb526c914f53c12c318bb4\n",
      "Successfully built librosa audioread resampy\n",
      "Installing collected packages: audioread, scipy, threadpoolctl, joblib, scikit-learn, llvmlite, numba, resampy, soundfile, appdirs, pooch, librosa\n",
      "Successfully installed appdirs-1.4.4 audioread-2.1.8 joblib-0.16.0 librosa-0.8.0 llvmlite-0.34.0 numba-0.51.2 pooch-1.2.0 resampy-0.2.2 scikit-learn-0.23.2 scipy-1.5.2 soundfile-0.10.3.post1 threadpoolctl-2.1.0\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.3.2-cp38-cp38-win_amd64.whl (8.5 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\miniconda\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.2.0-cp38-none-win_amd64.whl (58 kB)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\miniconda\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\miniconda\\lib\\site-packages (from matplotlib) (1.19.2)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-7.2.0-cp38-cp38-win_amd64.whl (2.1 MB)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\miniconda\\lib\\site-packages (from matplotlib) (2020.6.20)\n",
      "Requirement already satisfied: six>=1.5 in c:\\miniconda\\lib\\site-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
      "Installing collected packages: kiwisolver, cycler, pillow, matplotlib\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.2.0 matplotlib-3.3.2 pillow-7.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement timeit (from versions: none)\n",
      "ERROR: No matching distribution found for timeit\n"
     ]
    }
   ],
   "source": [
    "!pip install \"numpy\"\n",
    "!pip install \"pandas\"\n",
    "!pip install \"librosa\"\n",
    "!pip install \"matplotlib\"\n",
    "!pip install \"timeit\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import timeit\n",
    "import datetime\n",
    "\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Setting data directories <a class=\"anchor\" id=\"1.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory of spectrograms: ./data/spect_subsample_5x10s_np\n"
     ]
    }
   ],
   "source": [
    "ds_description = '5x10s'\n",
    "# Set the directory for the spectrograms\n",
    "data_dir = f'./data/spect_subsample_{ds_description}_np'\n",
    "print(\"Directory of spectrograms: {}\".format(data_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Defining functions <a class=\"anchor\" id=\"1.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir, ds_description, str_X, str_Y):\n",
    "    '''\n",
    "    Loads the .npy data files generated previously from the pre-processing ipynb\n",
    "    Note: .npy files need to be in the format: train_spect_{ds_description}_np.npy\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    data_dir: directory of the .npy files\n",
    "    ds_description: e.g. '5x10s'  5 subsamples of 10s length\n",
    "    str_X: str name of the 'X' data, either: 'spect' or 'X'\n",
    "    str_Y: str name of the 'Y' data, either: 'labels' or 'Y'\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    6 numpy arrays of:\n",
    "        train_{str_X}, train_{str_Y}, val_{str_X}, val_{str_Y}, test_{str_X}, test_{str_Y}\n",
    "    '''\n",
    "    assert (str_X in ['spect','X']), \"Assertion Error, str_X must be either 'spect' or 'X'.\"\n",
    "    assert (str_Y in ['labels','Y']), \"Assertion Error, str_Y must be either 'labels' or 'Y'.\"\n",
    "    \n",
    "    print(\"Loading .npy data files...\")\n",
    "    # Start timer\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    train_str_X = np.load(f'{data_dir}/train_{str_X}_{ds_description}_np.npy')\n",
    "    val_str_X = np.load(f'{data_dir}/val_{str_X}_{ds_description}_np.npy')\n",
    "    test_str_X = np.load(f'{data_dir}/test_{str_X}_{ds_description}_np.npy')\n",
    "\n",
    "    train_str_Y = np.load(f'{data_dir}/train_{str_Y}_{ds_description}_np.npy')\n",
    "    val_str_Y = np.load(f'{data_dir}/val_{str_Y}_{ds_description}_np.npy')\n",
    "    test_str_Y = np.load(f'{data_dir}/test_{str_Y}_{ds_description}_np.npy')\n",
    "    \n",
    "    elapsed = str(datetime.timedelta(seconds = timeit.default_timer() - start_time))\n",
    "    print(\"\", end='\\n')\n",
    "    print(\"Total processing time (h:mm:ss): {}\".format(elapsed[:-7]))\n",
    "    print(\"\\nLoaded .npy data files, verifying shape of saved data...\")\n",
    "    print(f\"Shape of 'train_{str_X}':\", train_str_X.shape)\n",
    "    print(f\"Shape of 'train_{str_Y}':\", train_str_Y.shape)\n",
    "\n",
    "    print(f\"Shape of 'val_{str_X}':\", val_str_X.shape)\n",
    "    print(f\"Shape of 'val_{str_Y}':\", val_str_Y.shape)\n",
    "\n",
    "    print(f\"Shape of 'test_{str_X}':\", test_str_X.shape)\n",
    "    print(f\"Shape of 'test_{str_Y}':\", test_str_Y.shape)\n",
    "    \n",
    "    return train_str_X, train_str_Y, val_str_X, val_str_Y, test_str_X, test_str_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaler_3d(array_3d):\n",
    "    '''\n",
    "    Takes in a 3D numpy array, converts to 2D to apply scikit-learn's \n",
    "    preprocessing.MinMaxScaler() method, and then converts to 3D\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    3D numpy array with values [0,1] (scaled with MinMaxScaler)    \n",
    "    '''\n",
    "    (s0, s1, s2) = array_3d.shape\n",
    "    array_2d = np.reshape(array_3d, (s0 * s1, s2))\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    array_2d = min_max_scaler.fit_transform(array_2d)\n",
    "    array_3d = np.reshape(array_2d, (s0, s1, s2))\n",
    "    \n",
    "    return array_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_classes(data_labels, class_dict):\n",
    "    '''\n",
    "    Takes in a 1D numpy array of labels, and converts to a 2D array of labels\n",
    "    based on the class_dict\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "    Class_dict: dictionary of int keys starting from 0, and str label values\n",
    "        e.g. genre_dict = {0 : 'Hip-Hop', 1 : 'Pop', 2 : 'Folk',}\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data_classified: 2D numpy int array of [0,1], 1 indicating the class for the jth location.\n",
    "    '''\n",
    "    # Reverse the dict to have str as the keys\n",
    "    class_dict_reverse = {v:k for k,v in class_dict.items()}\n",
    "    n_obs = len(data_labels)\n",
    "    n_cls = len(class_dict)\n",
    "    data_classified = np.zeros((n_obs, n_cls), dtype=int)\n",
    "    \n",
    "    for i in range(n_obs):\n",
    "        data_classified[i][class_dict_reverse[data_labels[i]]] = 1\n",
    "    \n",
    "    return data_classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    '''\n",
    "    Shuffles two arrays in unison along the first axis by using a permutation\n",
    "    Returns\n",
    "    -------\n",
    "    a and b numpy arrays shuffled in unison\n",
    "    '''\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Further Preprocessing <a class=\"anchor\" id=\"2.\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Reading in train, validation, and test data sets <a class=\"anchor\" id=\"2.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading .npy data files...\n",
      "\n",
      "Total processing time (h:mm:ss): 0:00:15\n",
      "\n",
      "Loaded .npy data files, verifying shape of saved data...\n",
      "Shape of 'train_spect': (31970, 431, 128)\n",
      "Shape of 'train_labels': (31970,)\n",
      "Shape of 'val_spect': (4000, 431, 128)\n",
      "Shape of 'val_labels': (4000,)\n",
      "Shape of 'test_spect': (4000, 431, 128)\n",
      "Shape of 'test_labels': (4000,)\n"
     ]
    }
   ],
   "source": [
    "# Read in the spectrogram and labels data from the .npy files\n",
    "train_spect, train_labels, val_spect, val_labels, test_spect, test_labels = load_data(\n",
    "    data_dir, ds_description, 'spect', 'labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train [min, max]: [-80.0, 3.814697265625e-06]\n",
      "Val [min, max]: [-80.0, 3.814697265625e-06]\n",
      "Test [min, max]: [-80.0, 3.814697265625e-06]\n"
     ]
    }
   ],
   "source": [
    "# Checking if all train, val, test sets have same min and max ranges\n",
    "print(\"Train [min, max]:\", [train_spect.min(), train_spect.max()])\n",
    "print(\"Val [min, max]:\", [val_spect.min(), val_spect.max()])\n",
    "print(\"Test [min, max]:\", [test_spect.min(), test_spect.max()])\n",
    "\n",
    "assert(train_spect.min() == val_spect.min() == test_spect.min()), 'minimum values do not match'\n",
    "assert(train_spect.max() == val_spect.max() == test_spect.max()), 'maximum values do not match'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Scaling the spectrograms for min max <a class=\"anchor\" id=\"2.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_spect_minmax = min_max_scaler_3d(val_spect)\n",
    "test_spect_minmax = min_max_scaler_3d(test_spect)\n",
    "train_spect_minmax = min_max_scaler_3d(train_spect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of 'train_spect': (31970, 431, 128)\n",
      "Train [min, max]: [0.0, 1.0]\n",
      "\n",
      "Shape of 'val_spect': (4000, 431, 128)\n",
      "Val [min, max]: [0.0, 1.0000000000000002]\n",
      "\n",
      "Shape of 'test_spect': (4000, 431, 128)\n",
      "Test [min, max]: [0.0, 1.0000000000000002]\n"
     ]
    }
   ],
   "source": [
    "# Cheecking shape and min and max values to be between 0 and 1\n",
    "print(\"Shape of 'train_spect':\", train_spect_minmax.shape)\n",
    "print(\"Train [min, max]:\", [train_spect_minmax.min(), train_spect_minmax.max()])\n",
    "print()\n",
    "print(\"Shape of 'val_spect':\", val_spect_minmax.shape)\n",
    "print(\"Val [min, max]:\", [val_spect_minmax.min(), val_spect_minmax.max()])\n",
    "print()\n",
    "print(\"Shape of 'test_spect':\", test_spect_minmax.shape)\n",
    "print(\"Test [min, max]:\", [test_spect_minmax.min(), test_spect_minmax.max()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Setting genre classes <a class=\"anchor\" id=\"2.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_dict = {0 : 'Hip-Hop',\n",
    "              1 : 'Pop',\n",
    "              2 : 'Folk',\n",
    "              3 : 'Experimental',\n",
    "              4 : 'Rock',\n",
    "              5 : 'International',\n",
    "              6 : 'Electronic',\n",
    "              7 : 'Instrumental'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map labels to classes\n",
    "train_classes = map_classes(train_labels, genre_dict)\n",
    "val_classes = map_classes(val_labels, genre_dict)\n",
    "test_classes = map_classes(test_labels, genre_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Saving Pre-Processed Data <a class=\"anchor\" id=\"3.\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Shuffling the data and saving as .npy files<a class=\"anchor\" id=\"3.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the data and save the the pre-processed 'X' and 'Y' data files\n",
    "\n",
    "train_X, train_Y = unison_shuffled_copies(train_spect_minmax, train_classes)\n",
    "val_X, val_Y = unison_shuffled_copies(val_spect_minmax, val_classes)\n",
    "test_X, test_Y = unison_shuffled_copies(test_spect_minmax, test_classes)\n",
    "\n",
    "np.save(f'{data_dir}/train_X_{ds_description}_np', train_X)\n",
    "np.save(f'{data_dir}/val_X_{ds_description}_np', val_X)\n",
    "np.save(f'{data_dir}/test_X_{ds_description}_np', test_X)\n",
    "\n",
    "np.save(f'{data_dir}/train_Y_{ds_description}_np', train_Y)\n",
    "np.save(f'{data_dir}/val_Y_{ds_description}_np', val_Y)\n",
    "np.save(f'{data_dir}/test_Y_{ds_description}_np', test_Y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
